


# ğŸ“§ AI Email-Drafter Agent (GPT-2 Medium + LoRA)

**Author:** Daksh Yadav  
**University:** Indian Institute of Technology (IIT) Mandi  
**Department:** Bioengineering  
**Date:** September 15, 2025  

---

## ğŸ“Œ Project Summary

The **AI Email-Drafter Agent** is a lightweight prototype that fine-tunes a transformer language model to generate polite, structured academic emails (e.g., extension requests, recommendation requests, leave requests, clarification emails).  

To make training feasible on commodity GPUs, the project uses **LoRA (PEFT)** adapters on top of **GPT-2 Medium** (causal LM) so that only small adapter weights are trained and stored.

### ğŸ¯ Goals
- Generate emails with a subject, greeting, body and polite closing.  
- Maintain a small storage footprint (LoRA adapters vs full model checkpoints).  
- Provide deterministic post-processing to enforce structure and reduce failure modes.  
- Log full interaction history (prompt â†’ raw output â†’ cleaned output) for reproducibility.  

---

## ğŸ“‚ Deliverables in this Repository

```

Ai-Email-Agent/
â”œâ”€â”€ demo/                     # screenshots + video demo
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ architecture.pdf      # architecture document (components, flow, model choices)
â”‚   â”œâ”€â”€ Data_Science_Report.pdf # data science report: fine-tuning setup + evaluation
â”‚   â””â”€â”€ interaction_logs.csv  # full eval interaction logs (prompt, raw, cleaned, flag)
â”œâ”€â”€ Source Code - Python Notebook/  # Colab notebooks and supporting scripts
â”œâ”€â”€ Project.ipynb - Colab.pdf # exported notebook PDF
â”œâ”€â”€ train_example.jsonl       # sample training dataset
â”œâ”€â”€ valid_example.jsonl       # sample validation / eval dataset
â”œâ”€â”€ loss_curves.png           # training loss chart (example)
â”œâ”€â”€ requirements.txt          # pip dependencies

â””â”€â”€ README.md                # this document




```

---

## ğŸ› ï¸ Design Rationale

- **Base model:** `gpt2-medium` (~345M parameters). Provides a balance of fluency and trainability on a single 8â€“16 GB GPU.  
- **PEFT / LoRA:** trains small adapters, making the process efficient and storage-friendly.  
- **Tokenizer:** GPT-2 BPE tokenizer with a `[PAD]` token for batching.  
- **Post-processing:** `cleanup_and_validate()` ensures presence of subject, greeting, closing, and normalizes years.  
- **UI:** Gradio-based demo for simple interaction and deployment.  

---

## ğŸš€ Quick Start â€” Google Colab

1. Open the main notebook in `Source Code - Python Notebook/` or `Project.ipynb` in **Google Colab**.  
2. Switch runtime to **GPU** (*Runtime â†’ Change runtime type â†’ GPU*).  
3. Install dependencies:  
   ```bash
   !pip install -r requirements.txt
   ```  
4. Run notebook cells in order:  
   - Data preparation  
   - Fine-tuning (Trainer + LoRA)  
   - Evaluation + log generation  
   - Demo UI (Gradio)  

---

## ğŸ’» Quick Start â€” Local Setup

1. Clone this repo:
   ```bash
   git clone https://github.com/<your-username>/Ai-Email-Agent.git
   cd Ai-Email-Agent
   ```

2. Setup environment:
   ```bash
   python -m venv venv
   source venv/bin/activate    # macOS/Linux
   venv\Scripts\activate     # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Run notebooks or scripts for training/evaluation.

---

## ğŸ“Š Training

Example command for fine-tuning:
```bash
python src/train_lora.py   --train_file data/train.jsonl   --validation_file data/valid.jsonl   --output_dir lora-output   --model_name_or_path gpt2-medium   --per_device_train_batch_size 1   --gradient_accumulation_steps 8   --num_train_epochs 8   --learning_rate 2e-4   --fp16
```

Or run training directly in Colab.  

---

## âœ… Evaluation & Interaction Logs

1. Prepare `eval.jsonl` in the format:  
   ```json
   {"prompt": "Instruction: ...", "completion": "Subject: ..."}
   ```

2. Run evaluation to generate:  
   ```
   reports/interaction_logs.csv
   ```  

This file contains:  
`prompt, reference_completion, raw_generation, cleaned_generation, valid_flag, settings`

---

## ğŸ¨ Demo UI (Gradio)

Run the Gradio app:
```bash
python src/ui_gradio.py --adapter_dir lora-output --model_name gpt2-medium
```

Or run the Gradio cell in Colab.  

---

## ğŸ“ˆ Results

- Training loss: **0.0751**  
- Validation loss: **0.0693**  
- Validation perplexity: **~1.072**  
- Structural correctness pass rate: **85â€“90%**  

---

## ğŸ“‘ Reports

- `architecture.pdf` â†’ AI agent architecture (components, flow, model choices).  
- `Data_Science_Report.pdf` â†’ dataset, preprocessing, fine-tuning setup, results, evaluation.  
- `interaction_logs.csv` â†’ full evaluation logs.  


