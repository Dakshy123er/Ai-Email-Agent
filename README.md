# ðŸ“§ AI Email-Drafter Agent (GPT-2 Medium + LoRA)

**Author:** Daksh Yadav  
**University:** Indian Institute of Technology (IIT) Mandi  
**Department:** Bioengineering  
**Date:** September 15, 2025

---

## Project Summary

The **AI Email-Drafter Agent** is a lightweight prototype that fine-tunes a transformer language model to generate polite, structured academic emails (e.g., extension requests, recommendation requests, leave requests, clarification emails).  

To make training feasible on commodity GPUs, the project uses **LoRA (PEFT)** adapters on top of **GPT-2 Medium** (causal LM) so that only small adapter weights are trained and stored.

### Goals
- Produce email drafts with a subject, greeting, body and polite closing.  
- Maintain a small storage footprint (LoRA adapters vs full model checkpoints).  
- Provide deterministic post-processing to enforce structure and reduce failure modes.  
- Log full interaction history (prompt â†’ raw output â†’ cleaned output) for reproducibility.  

---

## Deliverables in this Repository

Ai-Email-Agent/
â”œâ”€â”€ demo/ # screenshots + video demo
â”œâ”€â”€ reports/
â”‚ â”œâ”€â”€ architecture.pdf # architecture document (components, flow, model choices)
â”‚ â”œâ”€â”€ Data_Science_Report.pdf # data science report: fine-tuning setup + evaluation
â”‚ â””â”€â”€ interaction_logs.csv # full eval interaction logs (prompt, raw, cleaned, flag)
â”œâ”€â”€ Source Code - Python Notebook/ # Colab notebooks and supporting scripts
â”œâ”€â”€ Project.ipynb - Colab.pdf # exported notebook PDF
â”œâ”€â”€ train_example.jsonl # sample training dataset
â”œâ”€â”€ valid_example.jsonl # sample validation / eval dataset
â”œâ”€â”€ loss_curves.png # training loss chart (example)
â”œâ”€â”€ requirements.txt # pip dependencies
â””â”€â”€ README.md # this document

markdown
Copy code

---

## Design Rationale

- **Base model:** `gpt2-medium` (â‰ˆ345M parameters). Chosen for good fluency while being runnable on a single 8â€“16 GB GPU with PEFT.  
- **PEFT / LoRA:** training only small low-rank adapters â†’ low compute & storage cost.  
- **Tokenizer:** GPT-2 BPE tokenizer + `PAD` token for batching.  
- **Post-processing:** `cleanup_and_validate()` enforces `Subject:`, `Dear <Prof>`, polite closing, and year normalization.  
- **UI:** Gradio â†’ lightweight and easy demo deployment.  

---

## Quick Start â€” Colab

1. Open the main notebook in `Source Code - Python Notebook/` or `Project.ipynb` in Google Colab.  
2. Switch runtime to **GPU**: *Runtime â†’ Change runtime type â†’ GPU*.  
3. Install dependencies:
   ```python
   !pip install -r requirements.txt
Run the notebook cells in order:

Data preparation

Fine-tuning (Trainer + LoRA)

Evaluation + log generation

Demo UI (Gradio)

Quick Start â€” Local
Clone repo:

bash
Copy code
git clone https://github.com/<your-username>/Ai-Email-Agent.git
cd Ai-Email-Agent
Setup environment:

bash
Copy code
python -m venv venv
source venv/bin/activate    # macOS/Linux
venv\Scripts\activate       # Windows
Install dependencies:

bash
Copy code
pip install -r requirements.txt
Run notebook or scripts for training/evaluation.

Training
Example training command:

bash
Copy code
python src/train_lora.py \
  --train_file data/train.jsonl \
  --validation_file data/valid.jsonl \
  --output_dir lora-output \
  --model_name_or_path gpt2-medium \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 8 \
  --num_train_epochs 8 \
  --learning_rate 2e-4 \
  --fp16
Or simply run the notebook cells.

Evaluation & Interaction Logs
Prepare eval.jsonl like:

json
Copy code
{"prompt": "Instruction: ...", "completion": "Subject: ..."}
Run evaluation script/notebook to produce reports/interaction_logs.csv.

File contains: prompt, reference_completion, raw_generation, cleaned_generation, valid_flag, settings.

Demo UI (Gradio)
Run:

bash
Copy code
python src/ui_gradio.py --adapter_dir lora-output --model_name gpt2-medium
Or use the Gradio cell in the Colab notebook.

Results
Training loss: 0.0751

Validation loss: 0.0693

Validation perplexity: ~1.072

Pass rate (structural correctness): ~85â€“90%

Reports
architecture.pdf â†’ AI agent architecture, components, flow, model choices.

Data_Science_Report.pdf â†’ dataset, preprocessing, fine-tuning setup, results, evaluation.

interaction_logs.csv â†’ full evaluation logs.
